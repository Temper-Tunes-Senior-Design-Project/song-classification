{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c770d3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "import firebase_admin\n",
    "from firebase_admin import credentials\n",
    "from firebase_admin import firestore\n",
    "\n",
    "\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import pickle\n",
    "# #this folder location will be changed to DB location in actual cloud function\n",
    "# from os import chdir\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import element\n",
    "from copy import deepcopy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "929e35ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "moods = ['sad','angry','energetic','excited','happy','content','calm','depressed'] #Represents DB indexing of moods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b039a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup Spotify and Firebase Credentials\n",
    "sp = None\n",
    "def spotify_client():\n",
    "    global sp\n",
    "    sp_cred = None\n",
    "    with open('spotify_credentials.json') as credentials:\n",
    "        sp_cred = json.load(credentials)\n",
    "    sp = spotipy.Spotify(client_credentials_manager=SpotifyClientCredentials(sp_cred[\"client_id\"],sp_cred['client_secret']))\n",
    "\n",
    "\n",
    "cred,db = None,None\n",
    "def firestoreConnection():\n",
    "    global cred\n",
    "    global db\n",
    "    cred = credentials.Certificate(\"mood-swing-6c9d0-firebase-adminsdk-9cm02-66f39cc0dd.json\")\n",
    "    if not firebase_admin._apps:\n",
    "        firebase_admin.initialize_app(cred)\n",
    "    db = firestore.client()\n",
    "    \n",
    "MLP_model = None\n",
    "def load_mlp_model():\n",
    "    global MLP_model\n",
    "    with open('MLP2.pkl','rb') as f:\n",
    "        MLP_model = pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a6d6152",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'recommendations'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/gr/vj3qw3x52hjdxfj797fk839w0000gn/T/ipykernel_38037/667729887.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtracks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecommendations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed_tracks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack_uris\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tracks'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'recommendations'"
     ]
    }
   ],
   "source": [
    "tracks = sp.recommendations(seed_tracks=track_uris[:5], limit=50)['tracks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b14392f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_ids = [track[\"id\"] for track in tracks if track[\"id\"] not in seen_song_id_set]\n",
    "spotify_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2e4d03fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/uriyasabah/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator StandardScaler from version 1.1.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "features_df = retrieveTrackFeatures(track_ids)\n",
    "processed_features_dict = clipAndNormalize(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9b2e3284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "[0.06293352 0.03052539 0.01199036 0.21869232 0.54686192 0.01923839\n",
      " 0.00387516 0.10588294]\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "f = None\n",
    "for features in processed_features_dict.values():\n",
    "    if i == 0:\n",
    "        f = features\n",
    "        (pred, pred_probability) = getMoodLabelMLP([features])\n",
    "        print(pred[0])\n",
    "        print(pred_probability[0])\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0d93de2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Lifeline - 2003 Remaster', 'artist(s)': ['Spandau Ballet']}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make a dictionary of song titles and artist names\n",
    "scraperInputs = getTitlesAndArtists(track_ids)\n",
    "scraperInputs[\"6ClYObDPRsRwd4zVS8NAoO\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade2d6b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "689f4b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateNewSongs(mood, num_songs, closest_song_ids): ###user_id???\n",
    "    global sp\n",
    "    if sp == None:\n",
    "        spotify_client()\n",
    "        firestoreConnection()\n",
    "    \n",
    "    # Use Spotipy to retrieve track information\n",
    "    track_info = sp.tracks(closest_song_ids)['tracks']\n",
    "    # Remove any elements that are None\n",
    "    track_info = [track for track in track_info if track is not None]\n",
    "    # Extract the URIs from the track information\n",
    "    track_uris = [track['uri'] for track in track_info]\n",
    "    final_song_ids = []\n",
    "    while len(final_song_ids) < num_songs:\n",
    "        # Get recommended tracks and add them to final_song_ids if mood is classified as specified mood\n",
    "        tracks = sp.recommendations(seed_tracks=track_uris[:5], limit=1)['tracks'] ##############################LIMIT\n",
    "        track_ids = [track[\"id\"] for track in tracks if track[\"id\"]]\n",
    "        #Split ids by whether they are already labelled or not\n",
    "        old_track_ids = getAlreadyLabelled(track_ids) #WHAT TO DO WITH THESE???? MAYBE COMBINE AS LABELED_IDS after?\n",
    "        new_track_ids = [track_id for track_id in track_ids if track_id not in old_track_ids.keys()]\n",
    "        # Get song features of the new ids\n",
    "        features_df = retrieveTrackFeatures(new_track_ids)\n",
    "        # Preprocess features\n",
    "        features_dict = clipAndNormalize(features_df)\n",
    "        \n",
    "        # Make a dictionary of song titles and artist names\n",
    "        scraper_inputs = getTitlesAndArtists(new_track_ids)\n",
    "        \n",
    "        # Dictionary for lyrics of songs that are found\n",
    "        all_lyrics_dict = getScrapedLyrics(scraper_inputs)\n",
    "        \n",
    "        overlap_keys = [key for key in features_dict.keys() if key in all_lyrics_dict.keys()]\n",
    "        only_features = [key for key in features_dict.keys() if key not in overlap_keys]\n",
    "        only_lyrics = [key for key in all_lyrics_dict.keys() if key not in overlap_keys]\n",
    "        \n",
    "        #3. Get predictions and update database\n",
    "\n",
    "        #3a. if there are no features or lyrics, return an error\n",
    "        if len(features_dict.keys()) == 0 and len(all_lyrics_dict.keys()) == 0:\n",
    "                #probably just return a flag that says there are no features or lyrics not the response here\n",
    "                return \"songs found but no data available for predictions\"\n",
    "        else:\n",
    "                predictions = {}\n",
    "                #for first version, tokenize the lyrics and then pass then to the model inside the for loop\n",
    "                for key in overlap_keys: #- could probably be done in batches regardless\n",
    "                    MLP_pred, MLP_pred_probability = getMoodLabelMLP(features_dict[key])\n",
    "\n",
    "                    BERT_pred, MLP_flag = getOnlyMoodLabelFromLyrics(all_lyrics_dict[key])\n",
    "                    if MLP_pred == BERT_pred or MLP_flag == True:\n",
    "                        prediction = MLP_pred\n",
    "                    else:\n",
    "                        model_pred_diffs = (MLP_pred - BERT_pred)\n",
    "                        if MLP_pred > BERT_pred:\n",
    "                            sum_probabilities = MLP_pred + model_pred_diffs\n",
    "                        else:\n",
    "                            sum_probabilities = MLP_pred - model_pred_diffs\n",
    "                        #if sum_probabilities outside of below 0, then do 8-sum_probabilities\n",
    "                        if sum_probabilities < 0:\n",
    "                            prediction = 8 + sum_probabilities\n",
    "                        elif sum_probabilities > 7:\n",
    "                            prediction = sum_probabilities - 7\n",
    "                        else:\n",
    "                            prediction = sum_probabilities\n",
    "\n",
    "                    predictions[key]=prediction\n",
    "\n",
    "                for key in only_features:\n",
    "                        MLP_pred, MLP_pred_probability = getMoodLabelMLP([features_dict[key]])\n",
    "                        predictions[key]=MLP_pred\n",
    "\n",
    "                # Add song moods to DB\n",
    "                addTrackMoodToDB(predictions)\n",
    "                print(predictions)\n",
    "                \n",
    "                \n",
    "#                 DB.update_user_liked_songs(UID,predictions.keys()) \n",
    "                # ^^Need to add a check to discard songs no longer on spotify, \n",
    "                # otherwise we might recommend songs that are no longer on spotify\n",
    "\n",
    "#                 DB.update_last_login(UID,datetime.utcnow())\n",
    "\n",
    "        return \"success\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51b14dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#______________________________________________\n",
    "# Database Operations\n",
    "#______________________________________________\n",
    "            \n",
    "def getTrackMoodFromDB(track_id):\n",
    "    doc_ref = db.collection('songs').document(track_id)\n",
    "    doc_data = doc_ref.get()\n",
    "    if doc_data.exists:\n",
    "        return doc_data.to_dict().get('mood')\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def getAlreadyLabelled(track_ids):\n",
    "    already_labelled = {}\n",
    "    for track_id in track_ids:\n",
    "        mood = getTrackMoodFromDB(track_id)\n",
    "        if mood is not None:\n",
    "            already_labelled[track_id] = mood\n",
    "    return already_labelled\n",
    "\n",
    "def addTrackMoodToDB(tracks_dict):\n",
    "    for track_id, mood in tracks_dict.items():\n",
    "        print(f\"track_id: {track_id}\")\n",
    "        print(f\"track_id: {mood}\")\n",
    "        doc_ref = db.collection('songs').document(track_id)\n",
    "        doc_ref.set({\n",
    "            'mood': int(mood[0])\n",
    "        })\n",
    "    \n",
    "#______________________________________________\n",
    "# MLP Model Classifcation\n",
    "#______________________________________________\n",
    "\n",
    "def getMoodLabelMLP(songFeatures):\n",
    "    if MLP_model is None:\n",
    "        load_mlp_model()\n",
    "    prediction = MLP_model.predict(songFeatures)\n",
    "    pred_probability= MLP_model.predict_proba(songFeatures)\n",
    "    return prediction, pred_probability\n",
    "\n",
    "def retrieveTrackFeatures(track_ids):\n",
    "    dfs = []\n",
    "    for i in range(0, len(track_ids), 50):\n",
    "        # Retrieve track features with current offset\n",
    "        current_features = sp.audio_features(track_ids[i:i+50])\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(current_features)\n",
    "        \n",
    "        # Remove columns that we don't need\n",
    "        df = df.drop(['type', 'uri', 'analysis_url', 'track_href'], axis=1)\n",
    "        \n",
    "        \n",
    "        # Append to list of dataframes\n",
    "        dfs.append(df)\n",
    "    \n",
    "    # Concatenate all dataframes into a single one\n",
    "    features_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    \n",
    "    #convert to dictionary, with track id as key\n",
    "    #featuresDict = features_df.set_index('id').T.to_dict('list')\n",
    "    \n",
    "    return features_df\n",
    "\n",
    "def clipAndNormalize(features):\n",
    "    #clip the features to the range of the training data\n",
    "    features['danceability'] = features['danceability'].clip(lower=0.22718080000000002, upper=0.906)\n",
    "    features['energy'] = features['energy'].clip(lower=0.03545904, upper=0.978)\n",
    "    features['loudness'] = features['loudness'].clip(lower=-26.4981552, upper=-1.6015904000000007)\n",
    "    features['speechiness'] = features['speechiness'].clip(lower=0.0257, upper=0.46640959999999926)\n",
    "    features['acousticness'] = features['acousticness'].clip(lower=8.353136000000001e-05, upper=0.9884095999999992)\n",
    "    features['instrumentalness'] = features['instrumentalness'].clip(lower=0.0, upper=0.956)\n",
    "    features['liveness'] = features['liveness'].clip(lower=0.0494, upper=0.697)\n",
    "    features['valence'] = features['valence'].clip(lower=0.0382, upper=0.923)\n",
    "    features['tempo'] = features['tempo'].clip(lower=63.7631808, upper=188.00344319999996)\n",
    "    features['duration_ms'] = features['duration_ms'].clip(lower=88264.8768, upper=372339.1727999991)\n",
    "    features['time_signature'] = features['time_signature'].clip(lower=3.0, upper=5.0)\n",
    "    \n",
    "    #normalize the data\n",
    "    scaler = pickle.load(open('scaler2.pkl', 'rb'))\n",
    "    #fit on all columns except the track id\n",
    "    rawfeatures = features.drop(['id'], axis=1)\n",
    "    preprocessedFeatures = scaler.transform(rawfeatures)\n",
    "\n",
    "    #convert to dictionary, with track id as key\n",
    "    preprocessedFeatures = pd.DataFrame(preprocessedFeatures, columns=rawfeatures.columns)\n",
    "    preprocessedFeatures['id']= features['id']\n",
    "    preprocessedFeatures = preprocessedFeatures.set_index('id').T.to_dict('list')\n",
    "    return preprocessedFeatures\n",
    "\n",
    "\n",
    "#______________________________________________\n",
    "# Scraper Functions\n",
    "#______________________________________________\n",
    "\n",
    "def getTitlesAndArtists(track_ids):\n",
    "    titleArtistPairs = {}\n",
    "    for i in range(0,len(track_ids),50):\n",
    "        tracks = sp.tracks(track_ids[i:i+50])\n",
    "        for track in tracks['tracks']:\n",
    "            title=track['name']\n",
    "            #check if the track ends with (feat. artist) using a regex\n",
    "            if re.search(r' \\(feat. .*\\)$', title):\n",
    "                #remove the (feat. artist) from the title\n",
    "                title = re.sub(r' \\(feat. .*\\)$', '', title)\n",
    "\n",
    "            artists=[]\n",
    "            for artist in track['artists']:\n",
    "                artists.append(artist['name'])\n",
    "            titleArtistPairs[track['id']] = {'title':title,'artist(s)':artists}\n",
    "\n",
    "    return titleArtistPairs\n",
    "\n",
    "def getScrapedLyrics(scraperInputs):\n",
    "        all_lyrics_dict = {}\n",
    "        for id, songInfo in scraperInputs.items():\n",
    "                #maybe add a sleep or something to prevent getting blocked\n",
    "                lyrics = scrapeLyrics(songInfo['artist(s)'],songInfo['title'])\n",
    "                if len(lyrics) > 0:\n",
    "                        all_lyrics_dict[id]=lyrics\n",
    "        return all_lyrics_dict\n",
    "\n",
    "#Helps parse miscellaneous tags <i>, </br>, etc,.\n",
    "def _lyricsHelper(html, lyrics_list):\n",
    "    for tag in html.childGenerator():\n",
    "        if type(tag) == element.NavigableString:\n",
    "            _handleLyricAppend(lyrics_list, tag.text.strip())\n",
    "        elif tag.name == 'br' and lyrics_list[len(lyrics_list) - 1] != '':\n",
    "            lyrics_list.append('')\n",
    "        elif html.name == 'a':\n",
    "            _lyricsHelper(tag, lyrics_list)\n",
    "\n",
    "#Reads the HTML for lyrics dividers (if they exist) and appends the lyrics line by line to a list\n",
    "def _getLyricsFromHTML(html):\n",
    "    lyrics = html.findAll(\"div\", {\"data-lyrics-container\" : \"true\"})\n",
    "    lyrics_list = ['']\n",
    "    for segment in lyrics:\n",
    "        for a in segment.childGenerator():\n",
    "            lyric = None\n",
    "            if type(a) == element.NavigableString:\n",
    "                lyric = a.strip()\n",
    "                _handleLyricAppend(lyrics_list, lyric)\n",
    "            else:\n",
    "                _lyricsHelper(a, lyrics_list)\n",
    "            if a.name == 'br' and lyrics_list[len(lyrics_list) - 1] != '':\n",
    "                lyrics_list.append('')\n",
    "    return lyrics_list\n",
    "\n",
    "#Helper function to handle appending and manipulating lyrics_list. A new line is generated only for </br> tags\n",
    "def _handleLyricAppend(lyrics_list, lyric):\n",
    "    if lyric is not None:\n",
    "        last_index = len(lyrics_list) - 1\n",
    "        #Handle special cases (parenthesis and symbols stick with words for instance)\n",
    "        if lyrics_list[last_index] != '' and (lyrics_list[last_index][-1] in ['(','[','{','<'] or lyric in [')',']','}','>','!','?',',','.']):\n",
    "            lyrics_list[last_index] += lyric\n",
    "        else:\n",
    "            lyrics_list[last_index] += \" \" + lyric if lyrics_list[last_index] != '' else lyric\n",
    "\n",
    "#Determines whether a song will need to be translated (returns the link if it does, otherwise returns None)\n",
    "def _getSongTranslationLink(html):\n",
    "    translation_tags = html.find_all('a', {\"class\": re.compile('TextButton*')})\n",
    "    for tag in translation_tags:\n",
    "        if \"english-translations\" in tag['href']:\n",
    "            return tag['href']\n",
    "    return None\n",
    "\n",
    "#Determines whether a page exists\n",
    "def _pageExists(html):\n",
    "    return html.find('div', class_='render_404') == None\n",
    "        \n",
    "#function to scrape lyrics from genius, takes an array of artists, and songname\n",
    "def scrapeLyrics(artistnames, songname):\n",
    "    lyrics_list = []\n",
    "    found = False\n",
    "    i = 0\n",
    "    html = None\n",
    "    while i < len(artistnames) and not(found):\n",
    "        artistname = artistnames[i]\n",
    "        artistname2 = str(artistname.replace(' ','-')) if ' ' in artistname else str(artistname)\n",
    "        songname2 = str(songname.replace(' ','-')) if ' ' in songname else str(songname)\n",
    "        page_url = 'https://genius.com/'+ artistname2 + '-' + songname2 + '-' + 'lyrics'\n",
    "        page = requests.get(page_url)\n",
    "        html = BeautifulSoup(page.text, 'html.parser') \n",
    "        found = _pageExists(html)\n",
    "        i += 1\n",
    "    if found:\n",
    "        #check if there is an english translation\n",
    "        translation_url = _getSongTranslationLink(html)\n",
    "        if translation_url is not None:\n",
    "            page = requests.get(translation_url)\n",
    "            html = BeautifulSoup(page.text, 'html.parser') \n",
    "            lyrics_list = _getLyricsFromHTML(html)\n",
    "        else:\n",
    "            #If there isn't a translation, make sure it's in english in the first place\n",
    "            english = False\n",
    "            for script in html.findAll('script'):\n",
    "                if \"language\\\\\\\":\\\\\\\"en\" in str(script):\n",
    "                    english = True\n",
    "            if english: lyrics_list = _getLyricsFromHTML(html)\n",
    "    return lyrics_list\n",
    "\n",
    "\n",
    "#______________________________________________\n",
    "# BERT Sentiment Analysis Functions\n",
    "#______________________________________________\n",
    "\n",
    "def getOnlyMoodLabelFromLyrics(lyrics):\n",
    "    \n",
    "    #PART 1: DATA SETUP\n",
    "    moods = ['sad','angry','energetic','excited','happy','content','calm','depressed']\n",
    "    nums = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "    # create dictionary mapping strings to integers\n",
    "    mood_to_num = {mood: num for mood,num in zip(moods,nums)}\n",
    "    \n",
    "    #device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "    \n",
    "    #change to ./path/goemotions_model\n",
    "    BERT_model = AutoModelForSequenceClassification.from_pretrained(\"monologg/bert-base-cased-goemotions-original\",local_files_only=True)\n",
    "    #change to ./path/goemotions_tokenizer\n",
    "    BERT_Tokenizer = AutoTokenizer.from_pretrained(\"monologg/bert-base-cased-goemotions-original\",local_files_only=True)\n",
    "    emotionsAsValenceArousal= { 'admiration':(.6,.4),'amusement':(.6,.2),'anger':(-.8,.6),'annoyance':(-.6,.6),'approval':(.8,.6),'caring':(.6,-.2),'confusion':(-.2,.2),'curiosity':(0,.4),'desire':(.6,.6),'despair':(-.8,-.6),'disappointment':(-.6,-.6),'disapproval':(-.8,.65),'disgust':(-.8,.2),'embarrassment':(-.6,.4),'envy':(-.6,.4),'excitement':(.6,.8),'fear':(-.6,.8),'gratitude':(.6,-.6),'grief':(-.6,-.8),'gratitude':(.6,-.6),'grief':(-.6,-.8),'joy':(.8,.2),'love':(.8,.4),'nervousness':(-.4,.6),'optimism':(.6,.2),'pride':(.6,.1),'realization':(.2,.2),'relief':(.4,-.4),'remorse':(-.6,-.4),'sadness':(-.8,-.2),'surprise':(.2,.6),'neutral':(0,0)}\n",
    "\n",
    "    emotion_dict = BERT_model.config.id2label\n",
    "\n",
    "\n",
    "    #PART 2 - get the mood label\n",
    "    mood,relyOnLinearModel = getMoodLabelFromLyrics(lyrics,BERT_model, BERT_Tokenizer, emotion_dict, emotionsAsValenceArousal, device='cpu',printValenceArousal=False)\n",
    "    mood = mood_to_num[mood]\n",
    "    return mood,relyOnLinearModel\n",
    "\n",
    "\n",
    "def getMoodLabelFromLyrics(lyrics,model, tokenizer, emotion_dict, emotionsAsValenceArousal,printValenceArousal = False,disregardNeutral=True, printRawScores=False, printTopN=False,topScoresToPrint=3,max_length=512, device=\"cuda\",  returnSongSegments=False):\n",
    "    relyOnLinearResults = False\n",
    "    softmaxScoresPerHeader = {}\n",
    "    model.to(device)\n",
    "    \n",
    "    #part 1 - break up the lyrics into chunks and get the tokens\n",
    "    if returnSongSegments:\n",
    "        songTokenChunks,freqs,songSegs =breakUpSongByHeaders(lyrics,tokenizer,returnSongSegments=returnSongSegments,max_length=max_length, device=device)\n",
    "    else:\n",
    "        songTokenChunks,freqs =breakUpSongByHeaders(lyrics,tokenizer,returnSongSegments=returnSongSegments,max_length=max_length, device=device)\n",
    "\n",
    "    #part 2 - get the softmax score for each chunk\n",
    "\n",
    "    if len(songTokenChunks) == 1:\n",
    "        disregardNeutral=False\n",
    "\n",
    "    #softmax scores returns COMBINED SINGLE LABEL -- MAYBE TRY MULTIPLE LABELS AND TAKE THE MOST COMMON\n",
    "    for header,tokenChunksPerHeaders in songTokenChunks.items():\n",
    "        for tokenChunk in tokenChunksPerHeaders:\n",
    "            ## ^^ If I encode multiple songs in batches, then I would make another for loop here and not just use tokenChunk[0]\n",
    "            ## but it might be too complicated to do that this way.  \n",
    "            # I'd have to make a function that breaks up the lyrics into chunks, \n",
    "            # and then return the chunks in a way that we still know which chunk belongs to which song and header\n",
    "            if header not in softmaxScoresPerHeader:\n",
    "                softmaxScoresPerHeader[header] = getSoftmax(model,tokenizer,tokens=tokenChunk[0],n=topScoresToPrint, printTopN=printTopN, printRawScores=printRawScores,device=device)\n",
    "            else:\n",
    "                softmaxScoresPerHeader[header] += getSoftmax(model,tokenizer,tokens=tokenChunk[0],n=topScoresToPrint, printTopN=printTopN, printRawScores=printRawScores,device=device)\n",
    "            \n",
    "            \n",
    "    #Part 3 determine what to do with the neutral labels\n",
    "    moodLabel, valence, arousal = convertScoresToLabels(softmaxScoresPerHeader,freqs, emotionsAsValenceArousal,emotion_dict,disregardNeutral=disregardNeutral,printValenceArousal=printValenceArousal)\n",
    "\n",
    "    if moodLabel=='top ratings all neutral':\n",
    "        disregardNeutral=False\n",
    "        moodLabel, valence, arousal = convertScoresToLabels(softmaxScoresPerHeader,freqs, emotionsAsValenceArousal,emotion_dict,disregardNeutral=disregardNeutral,printValenceArousal=printValenceArousal)\n",
    "        relyOnLinearResults = True\n",
    "    if moodLabel=='neutral' or (-0.1<valence<0.1 and -0.1<arousal<0.1):\n",
    "        relyOnLinearResults = True\n",
    "    #part 4 - return the most common label\n",
    "    return moodLabel, relyOnLinearResults\n",
    "\n",
    "\n",
    "# input: a string of whole song\n",
    "# output: a dictionary of with header values and a list of tensors (sometmes more than 1 item) for each header chunk\n",
    "def breakUpSongByHeaders(songLines, tokenizer, max_length=512, device=\"cuda\",  returnSongSegments=False):\n",
    "    songSegmentsDict = {}\n",
    "    tokenSegmentsDict = {}\n",
    "    headerFreqsDict = {}\n",
    "\n",
    "    #strip the trailing whitespace\n",
    "    lines = [line.strip() for line in songLines]\n",
    "\n",
    "    #find the lines that start with [ and end with ]\n",
    "    headerLinesIndex = [i for i, line in enumerate(lines) if line.startswith('[') and line.endswith(']')]\n",
    "    #check for consecutive headers indexes and remove the first one\n",
    "    for i in range(len(headerLinesIndex)-1):\n",
    "        if headerLinesIndex[i+1] - headerLinesIndex[i] == 1:\n",
    "            headerLinesIndex[i] = -1\n",
    "    headerLinesIndex = [i for i in headerLinesIndex if i != -1]\n",
    "\n",
    "    for i in range(len(headerLinesIndex)):\n",
    "        header_line = lines[headerLinesIndex[i]][1:-1]  # remove square brackets\n",
    "        if header_line in songSegmentsDict:\n",
    "            songSegmentsDict[header_line][0] += 1\n",
    "        elif i == len(headerLinesIndex)-1:\n",
    "            songSegmentsDict[header_line] = [1, \" \".join(lines[headerLinesIndex[i]+1:]), lines[headerLinesIndex[i]+1:]]\n",
    "        else:\n",
    "            songSegmentsDict[header_line] = [1, \" \".join(lines[headerLinesIndex[i]+1:headerLinesIndex[i+1]]), lines[headerLinesIndex[i]+1:headerLinesIndex[i+1]]]\n",
    "\n",
    "    for header, lyrics in songSegmentsDict.items():\n",
    "        if returnSongSegments:\n",
    "            tokenSegmentsDict[header],subLyrics = breakUpLargeLyricChunks(lyrics[1],lyrics[2],tokenizer,returnLyricsSegments=returnSongSegments,max_length=max_length, device=device)\n",
    "            songSegmentsDict[header]=subLyrics\n",
    "        else:\n",
    "            tokenSegmentsDict[header] = breakUpLargeLyricChunks(lyrics[1],lyrics[2],tokenizer,returnLyricsSegments=returnSongSegments,max_length=max_length, device=device)\n",
    "        headerFreqsDict[header] = lyrics[0]\n",
    "\n",
    "    if returnSongSegments:\n",
    "        return tokenSegmentsDict,headerFreqsDict,songSegmentsDict\n",
    "    else:\n",
    "        return tokenSegmentsDict,headerFreqsDict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def breakUpLargeLyricChunks(lyricsChunkString, lines,tokenizer, max_length=512, device=\"cuda\", returnLyricsSegments=False):\n",
    "    #lines = lyricsChunkString.splitlines()  # split the lyrics into lines\n",
    "    segments = []  # store the lyrics segments\n",
    "    token_segments = []  # store the tokenized segments as tensors\n",
    "\n",
    "    token_segment = tokenizer.encode(lyricsChunkString, return_tensors=\"pt\")#.to(device)\n",
    "\n",
    "    if len(token_segment[0]) <= max_length:\n",
    "        token_segment = token_segment.unsqueeze(0)\n",
    "        token_segments.append(token_segment)\n",
    "        segments.append(lyricsChunkString)\n",
    "    else:\n",
    "        # calculate the average number of lines per segment. Add +2 to ensure segments are not still too long\n",
    "        avg_lines_per_segment = len(lines) // ((len(token_segment[0]) // max_length) + 2)\n",
    "\n",
    "        # loop through the lines and group them into segments of roughly the same length\n",
    "        for start_idx in range(0, len(lines), avg_lines_per_segment):\n",
    "            end_idx = start_idx + avg_lines_per_segment\n",
    "\n",
    "            smallLastChunk = end_idx >= len(lines)-2\n",
    "            \n",
    "            if smallLastChunk:\n",
    "                segment = \" \".join(lines[start_idx:])\n",
    "            else:\n",
    "                segment = \" \".join(lines[start_idx:end_idx])\n",
    "            segments.append(segment)\n",
    "\n",
    "            # tokenize the segment and convert to tensor\n",
    "            token_segment = tokenizer.encode(segment, return_tensors=\"pt\")#.to(device)\n",
    "            token_segment = token_segment.unsqueeze(0)\n",
    "            token_segments.append(token_segment)\n",
    "            #NOTE: ^^ If I use batch_encode_plus, I can get the tokenized segments as a list of tensors in one step\n",
    "            #I would just have to do it after the loop. \n",
    "            #Since it is a small list though, I don't think it will make a difference in this case\n",
    "\n",
    "            if smallLastChunk:\n",
    "                #this is the last segment early, so break out of the loop\n",
    "                break\n",
    "\n",
    "    if returnLyricsSegments:  \n",
    "        return token_segments, segments\n",
    "    else:\n",
    "        return token_segments\n",
    "\n",
    "\n",
    "def getSoftmax(model,tokenizer, tokens = None, sentence=None, n=3,printRawScores=False, printTopN=False,device='cuda'):\n",
    "    if tokens is None:\n",
    "        tokens = tokenizer.encode(sentence, return_tensors=\"pt\")\n",
    "    if device=='cuda':\n",
    "        tokens = tokens.cuda()\n",
    "    result = model(tokens)\n",
    "    emotion = result.logits\n",
    "    emotion = emotion.cpu().detach().numpy()\n",
    "    emotion = emotion[0]\n",
    "    softmax = tf.nn.softmax(emotion)\n",
    "    #convert to numpy array\n",
    "    softmax = softmax.numpy()\n",
    "    if printRawScores:\n",
    "        print(softmax)\n",
    "    \n",
    "    if printTopN:\n",
    "        emotion = emotion.argsort()[-n:][::-1]\n",
    "        emotion = emotion.tolist()\n",
    "        printTopEmotions(emotion,model, softmax)\n",
    "    return softmax\n",
    "\n",
    "def printTopEmotions(emotion, model, softmax):\n",
    "    \n",
    "    #identify the label of top n emotions from emotion list\n",
    "    #softmax is in the order of the values in emotion_dict so we can use emotion[id] to get the softmax value\n",
    "    id=0\n",
    "    emotion_dict = model.config.id2label\n",
    "    for i in emotion:\n",
    "        print(emotion_dict[i])\n",
    "        print(softmax[emotion[id]]*100,\"%\")\n",
    "        id+=1\n",
    "    return\n",
    "\n",
    "\n",
    "def convertScoresToLabels(softmaxScoresPerHeader,headerFreqs, emotionsAsValenceArousal,emotion_dict,disregardNeutral = True, printValenceArousal=False,printTopChunkEmotions=False):\n",
    "    #convert the softmax scores to a valence and arousal score\n",
    "    #softmax scores are in the order of the values in emotion_dict so we can use emotion[id] to get the softmax value\n",
    "    valence=0\n",
    "    arousal=0\n",
    "    softmaxScoresApplied=0\n",
    "    #find the key in emotion_dict that corresponds to neutral\n",
    "    neuturalKey = [key for key, value in emotion_dict.items() if value == 'neutral'][0]\n",
    "    for key, softmaxScores in softmaxScoresPerHeader.items():\n",
    "        #check if neutral is the highest softmax score\n",
    "        if disregardNeutral and neuturalKey==softmaxScores.argmax():\n",
    "            continue\n",
    "        else:\n",
    "            #multiply the softmax score by the valence and arousal values and add to the total valence and arousal\n",
    "            #do this for the number in the headerFreqs dictionary\n",
    "            for i in range(headerFreqs[key]):\n",
    "                id=0\n",
    "                softmaxScoresApplied+=1\n",
    "                for i in softmaxScores:\n",
    "                    valence+=i*emotionsAsValenceArousal[emotion_dict[id]][0]\n",
    "                    arousal+=i*emotionsAsValenceArousal[emotion_dict[id]][1]\n",
    "                    id+=1\n",
    "    #divide the total valence and arousal by the number of softmax scores applied\n",
    "    if softmaxScoresApplied!=0:\n",
    "        valence=valence/softmaxScoresApplied\n",
    "        arousal=arousal/softmaxScoresApplied\n",
    "        mood =determineMoodLabel(valence,arousal,printValenceArousal=printValenceArousal)\n",
    "        return mood, valence, arousal\n",
    "    else:\n",
    "        return 'top ratings all neutral', valence, arousal\n",
    "    #note this means all top chunk emotions were neutral as opposed to true neutral where all emotions balance out to neutral\n",
    "\n",
    "def determineMoodLabel(valence,arousal,printValenceArousal=False):\n",
    "    #determine the diagonal of the circumplex model that the valence and arousal scores fall on\n",
    "    #MAKE 2 BOXES OF THE CIRCUMPLEX MODEL A MOOD \n",
    "\n",
    "    energetic =   -0.5<valence<0.5 and arousal>0.5\n",
    "    happy =       valence>0.5 and -.5<arousal<0.5\n",
    "    calm =       -0.5<valence<0.5 and arousal<-0.5\n",
    "    sad =         valence<-0.5 and -.5<arousal<0.5\n",
    "\n",
    "    excited =   not (happy or energetic) and valence>0 and arousal>0\n",
    "    content =   not (calm or happy) and valence>0 and arousal<0\n",
    "    depressed = not (calm or sad) and valence<0 and arousal<0\n",
    "    angry =   not (energetic or sad) and valence<0 and arousal>0\n",
    "\n",
    "\n",
    "    if energetic:\n",
    "        mood='energetic'\n",
    "    elif happy:\n",
    "        mood='happy'\n",
    "    elif calm:\n",
    "        mood='calm'\n",
    "    elif sad:\n",
    "        mood='sad'\n",
    "    elif excited:\n",
    "        mood='excited'\n",
    "    elif content:\n",
    "        mood='content'\n",
    "    elif depressed:\n",
    "        mood='depressed'\n",
    "    elif angry:\n",
    "        mood='angry'\n",
    "    else:\n",
    "        mood='neutral'\n",
    "    \n",
    "    if printValenceArousal:\n",
    "        print(\"Valence: \",valence)\n",
    "        print(\"Arousal: \",arousal)\n",
    "    return mood     \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8703eb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#________________________________________________________________________________________________________________\n",
    "\n",
    "\n",
    "#COMBINED MODEL OUTLINE -- \n",
    "# inputs:  sp_user( aka the client!!!), UID\n",
    "#^note: we could pass in the user's last login date, instead of UID if we waned to, \n",
    "# as this would provide beter separation of concerns of updating last login date vs the function \n",
    "# thats supposed to be getting the song moods\n",
    "\n",
    "#outputs: verifies success or failure of the function\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# def get_user_song_moods_advanced(sp_user,UID):\n",
    "\n",
    "#         #0. get the track ids of unlabelled songs from the list of new liked songs from the user\n",
    "#         last_login = DB.get_last_login(UID) #IF NONE, return ''\n",
    "\n",
    "#         #If last login is a string, we need to convert it to a utc datetime object\n",
    "#         #if isinstance(last_login, str):\n",
    "#         #        last_login = datetime.strptime(last_login, '%Y-%m-%d %H:%M:%S.%f')\n",
    "        \n",
    "#         track_ids = retrieveTrackIds(sp_user,last_login)\n",
    "\n",
    "#         remaining_track_ids = DB.check_if_already_labelled(track_ids)\n",
    "\n",
    "#         if len(remaining_track_ids) == 0:\n",
    "#                 DB.update_user_liked_songs(UID,track_ids)\n",
    "#                 DB.update_last_login(UID,datetime.utcnow())\n",
    "#                 return \"success\"\n",
    "\n",
    "#         #1. get the song features of unlabelled songs\n",
    "\n",
    "#         featuresDF = retrieveTrackFeatures(remaining_track_ids) #this should drop the ids of songs that dont have features\n",
    "#         featuresDict = clipAndNormalize(featuresDF)#.set_index('id').T.to_dict('list')\n",
    "\n",
    "#         #2. if possible, get the lyrics of the songs\n",
    "\n",
    "#         #make a dictionary of song titles and artist names\n",
    "#         scraperInputs = getTitlesAndArtists(sp_user, remaining_track_ids)\n",
    "\n",
    "\n",
    "#         #-------------------------------------------------\n",
    "#         #compute gets expensive here, so just do a for loop on remaining_track_ids\n",
    "#         #where u \n",
    "#         # 1. get lyrics, \n",
    "#         # 2. check if features are available as well \n",
    "#         # 3. make a prediction, based on 1 and 2 \n",
    "#         # 4. immediately update the database and last added_at date, in case of a time out\n",
    "#         #-------------------------------------------------\n",
    "#         all_lyrics_dict = getScrapedLyrics(scraperInputs)\n",
    "\n",
    "#         #MAYBE Tokenize THE LYRICS HERE (use batch processing?!) \n",
    "#         #would need to verify compute restrictions of cloud function first!\n",
    "\n",
    "#         overlap_keys = [key for key in featuresDict.keys() if key in all_lyrics_dict.keys()]\n",
    "#         only_features = [key for key in featuresDict.keys() if key not in overlap_keys]\n",
    "#         only_lyrics = [key for key in all_lyrics_dict.keys() if key not in overlap_keys]\n",
    "\n",
    "\n",
    "#         #3 get predictions and update database\n",
    "\n",
    "#         #3a. if there are no features or lyrics, return an error\n",
    "#         if len(featuresDict.keys()) == 0 and len(all_lyrics_dict.keys()) == 0:\n",
    "#                 #probably just return a flag that says there are no features or lyrics not the response here\n",
    "#                 return \"songs found but no data available for predictions\"\n",
    "        \n",
    "#         else:\n",
    "#                 predictions = {}\n",
    "#                 #for first version, tokenize the lyrics and then pass then to the model inside the for loop\n",
    "\n",
    "#                 for key in overlap_keys: #- could probably be done in batches regardless\n",
    "#                     MLP_pred, MLP_pred_probability = getMoodLabelMLP(featuresDict[key])\n",
    "\n",
    "#                     #BERT_pred, BERT_pred_probability = getMoodLabelBERT(BERT_model,all_lyrics_dict[key])\n",
    "#                     BERT_pred, MLP_flag = getOnlyMoodLabelFromLyrics(all_lyrics_dict[key])\n",
    "#                     if MLP_pred == BERT_pred or MLP_flag == True:\n",
    "#                         prediction = MLP_pred\n",
    "#                     else:\n",
    "                        \n",
    "#                         model_pred_diffs = (MLP_pred - BERT_pred)\n",
    "\n",
    "#                         if MLP_pred > BERT_pred:\n",
    "#                             sum_probabilities = MLP_pred + model_pred_diffs\n",
    "#                         else:\n",
    "#                             sum_probabilities = MLP_pred - model_pred_diffs\n",
    "#                         #if sum_probabilities outside of below 0, then do 8-sum_probabilities\n",
    "#                         if sum_probabilities < 0:\n",
    "#                             prediction = 8 + sum_probabilities\n",
    "#                         elif sum_probabilities > 7:\n",
    "#                             prediction = sum_probabilities - 7\n",
    "#                         else:\n",
    "#                             prediction = sum_probabilities\n",
    "\n",
    "#                     predictions[key]=prediction\n",
    "\n",
    "#                 for key in only_features:\n",
    "#                         MLP_pred, MLP_pred_probability = getMoodLabelMLP(featuresDict[key])\n",
    "#                         predictions[key]=MLP_pred\n",
    "\n",
    "#                 DB.add_song_moods(predictions)\n",
    "                \n",
    "#                 DB.update_user_liked_songs(UID,predictions.keys()) \n",
    "#                 # ^^Need to add a check to discard songs no longer on spotify, \n",
    "#                 # otherwise we might recommend songs that are no longer on spotify\n",
    "\n",
    "#                 DB.update_last_login(UID,datetime.utcnow())\n",
    "\n",
    "#         return \"success\"\n",
    "\n",
    "#________________________________________________________________________________________________________________\n",
    "### Helper Functions of get_User_Song_Moods_2_models\n",
    "#________________________________________________________________________________________________________________\n",
    "\n",
    "# def retrieveTrackIds(user_prior_login_date):\n",
    "#     track_ids = []\n",
    "#     offset = 0\n",
    "#     limit = 50\n",
    "#     liked_tracks = sp.current_user_saved_tracks(limit=limit, offset=offset)\n",
    "#     endLoopEarly = False\n",
    "#     while True:\n",
    "#         for item in liked_tracks['items']:\n",
    "#             if user_prior_login_date != '':\n",
    "#                 if datetime.strptime(item['added_at'],'%Y-%m-%dT%H:%M:%SZ')> user_prior_login_date:\n",
    "#                     track_ids.append(item['track']['id'])\n",
    "#                 else:\n",
    "#                     endLoopEarly = True\n",
    "#                     break\n",
    "#             else:\n",
    "#                 track_ids.append(item['track']['id'])\n",
    "#         offset += limit\n",
    "        \n",
    "#         if len(liked_tracks['items']) < limit or endLoopEarly:\n",
    "#             # All tracks have been retrieved\n",
    "#             break\n",
    "        \n",
    "#         liked_tracks = sp.current_user_saved_tracks(limit=limit, offset=offset)\n",
    "    \n",
    "#     return track_ids\n",
    "\n",
    "\n",
    "# '''def getSongFeaturesToDict(track_ids):\n",
    "#         featuresDict = {}\n",
    "#         for i in range(0,len(track_ids),100):\n",
    "#                 features = sp.audio_features(track_ids[i:i+100])\n",
    "#                 for feature in features:\n",
    "#                         if feature != None:\n",
    "#                                 featuresDict[feature['id']]=feature\n",
    "#         return featuresDict'''\n",
    "\n",
    "\n",
    "# def getMoodLabelMLP(songFeautures):\n",
    "#         with open('MLP1.pkl','rb') as f:\n",
    "#             model = pickle.load(f)\n",
    "#         prediction = model.predict(songFeautures)\n",
    "#         pred_probability=model.predict_proba(songFeautures)\n",
    "#         return prediction, pred_probability\n",
    "\n",
    "#________________________________________________________________________________________________________________\n",
    "#                  MAIN\n",
    "#________________________________________________________________________________________________________________\n",
    "\n",
    "def generate_song_classification_advanced(request):\n",
    "        spotify_access_token = request.args['spotify_token']\n",
    "        userID = request.args['uid']\n",
    "        client = loadSpotipyClient(spotify_access_token)\n",
    "\n",
    "        feedback = get_user_song_moods_advanced(client, userID)\n",
    "\n",
    "\n",
    "        #CORS-Policy Headers\n",
    "        headers = {\n",
    "        'Access-Control-Allow-Origin': '*',\n",
    "        'Access-Control-Allow-Methods': 'GET',\n",
    "        'Access-Control-Allow-Headers': 'Content-Type',\n",
    "        'Access-Control-Max-Age': '3600'\n",
    "        }\n",
    "        # Jsonify predictions\n",
    "        if feedback == \"success\":\n",
    "                return (jsonify({}), 200, headers)\n",
    "        else:\n",
    "                return (jsonify({\"warning\":feedback}), 200, headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0b17881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "track_id: 3Am4yxuwBTorY6kaqX31zE\n",
      "track_id: [7]\n",
      "{'3Am4yxuwBTorY6kaqX31zE': array([7], dtype=int8)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'success'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# user_id = \"\"\n",
    "mood = \"angry\"\n",
    "num_songs = 50\n",
    "closest_song_ids = ['4uLU6hMCjMI75M1A2tKUQC', '2takcwOaAZWiXQijPHIx7B', '0JrXhKXDZpcOzYUvV2fYhZ']\n",
    "generateNewSongs(mood, num_songs, closest_song_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014c4deb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
